{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook was executed on Kaggle:\n",
    "> https://www.kaggle.com/code/hemanthssr1/roman-to-devanagari-seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roman-to-Devanagari Seq2Seq\n",
    "\n",
    "This notebook implements a configurable character-level sequence-to-sequence model (PyTorch) to map romanized strings to Devanagari. It includes data loading from the `aksharantar_sampled` folder, preprocessing, a flexible Encoder/Decoder (RNN/LSTM/GRU), training loop, and analytic formulas for compute and parameter counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T21:21:10.513317Z",
     "iopub.status.busy": "2025-10-30T21:21:10.512910Z",
     "iopub.status.idle": "2025-10-30T21:21:10.524389Z",
     "shell.execute_reply": "2025-10-30T21:21:10.523587Z",
     "shell.execute_reply.started": "2025-10-30T21:21:10.513290Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device being used: cuda\n",
      "Preset: emb_dim=128, hidden_dim=256, num_layers=2, batch_size=128, max_len=48, rnn_type=GRU\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# For reproducibility: set random seeds so results are consistent\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Use GPU if available, otherwise fallback to CPU\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device being used:', DEVICE)\n",
    "\n",
    "# Preset hyperparameters for GPU vs CPU\n",
    "if DEVICE.type == 'cuda':\n",
    "    # GPU: bigger model, larger batch, longer sequence\n",
    "    EMB_DIM = 128\n",
    "    HIDDEN_DIM = 256\n",
    "    NUM_LAYERS = 2\n",
    "    BATCH_SIZE = 128\n",
    "    MAX_LEN = 48\n",
    "    RNN_TYPE = 'GRU'\n",
    "else:\n",
    "    # CPU: smaller model, smaller batch, shorter sequence\n",
    "    EMB_DIM = 64\n",
    "    HIDDEN_DIM = 128\n",
    "    NUM_LAYERS = 1\n",
    "    BATCH_SIZE = 32\n",
    "    MAX_LEN = 32\n",
    "    RNN_TYPE = 'LSTM'\n",
    "\n",
    "print(f\"Preset: emb_dim={EMB_DIM}, hidden_dim={HIDDEN_DIM}, num_layers={NUM_LAYERS}, batch_size={BATCH_SIZE}, max_len={MAX_LEN}, rnn_type={RNN_TYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T21:21:14.181742Z",
     "iopub.status.busy": "2025-10-30T21:21:14.181443Z",
     "iopub.status.idle": "2025-10-30T21:21:54.140665Z",
     "shell.execute_reply": "2025-10-30T21:21:54.139749Z",
     "shell.execute_reply.started": "2025-10-30T21:21:14.181719Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 911513 training pairs from 19 files\n",
      "Example 1: mwnlwnga -> मोनलोङा\n",
      "Example 2: ransargra -> रानसारग्रा\n",
      "Example 3: baohordwngmwn -> बावहरदोंमोन\n",
      "Example 4: riyel -> रियेल\n",
      "Example 5: tamkonayari -> थामखनायारि\n"
     ]
    }
   ],
   "source": [
    "# Load the Aksharantar dataset: we'll grab all *_train.csv files from each language folder\n",
    "DATA_DIR = '/kaggle/input/aksharantar/aksharantar_sampled'\n",
    "train_files = glob.glob(os.path.join(DATA_DIR, '*', '*_train.csv'))\n",
    "\n",
    "pairs = []  # Each pair is (romanized, devanagari)\n",
    "for f in train_files:\n",
    "    df = pd.read_csv(f, header=None, names=['latin','dev'])\n",
    "    # Remove any empty lines or NaNs\n",
    "    df = df.dropna()\n",
    "    for _, row in df.iterrows():\n",
    "        latin = str(row['latin']).strip()\n",
    "        dev = str(row['dev']).strip()\n",
    "        if latin and dev:\n",
    "            pairs.append((latin, dev))\n",
    "\n",
    "print(f'Loaded {len(pairs)} training pairs from {len(train_files)} files')\n",
    "# Let's peek at a few examples\n",
    "for i in range(5):\n",
    "    print(f'Example {i+1}: {pairs[i][0]} -> {pairs[i][1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T21:18:37.214544Z",
     "iopub.status.busy": "2025-10-30T21:18:37.214268Z",
     "iopub.status.idle": "2025-10-30T21:18:38.515519Z",
     "shell.execute_reply": "2025-10-30T21:18:38.514569Z",
     "shell.execute_reply.started": "2025-10-30T21:18:37.214522Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source vocab size: 29\n",
      "Target vocab size: 681\n",
      "First few source vocab chars: ['<pad>', '<s>', '</s>', 'a', 'b', 'c', 'd', 'e', 'f', 'g']\n",
      "First few target vocab chars: ['<pad>', '<s>', '</s>', '،', 'ؐ', 'ء', 'آ', 'ؤ', 'ئ', 'ا']\n"
     ]
    }
   ],
   "source": [
    "# Build character-level vocabularies and tokenizers for both scripts\n",
    "# We'll use special tokens for start-of-sequence, end-of-sequence, and padding\n",
    "SOS_token = '<s>'\n",
    "EOS_token = '</s>'\n",
    "PAD_token = '<pad>'\n",
    "\n",
    "# Gather all unique characters in source (romanized) and target (devanagari)\n",
    "src_chars = set()\n",
    "tgt_chars = set()\n",
    "for src, tgt in pairs:\n",
    "    src_chars.update(list(src))\n",
    "    tgt_chars.update(list(tgt))\n",
    "\n",
    "src_vocab = [PAD_token, SOS_token, EOS_token] + sorted(src_chars)\n",
    "tgt_vocab = [PAD_token, SOS_token, EOS_token] + sorted(tgt_chars)\n",
    "\n",
    "src_char2idx = {c: i for i, c in enumerate(src_vocab)}\n",
    "src_idx2char = {i: c for c, i in src_char2idx.items()}\n",
    "tgt_char2idx = {c: i for i, c in enumerate(tgt_vocab)}\n",
    "tgt_idx2char = {i: c for c, i in tgt_char2idx.items()}\n",
    "\n",
    "print(f'Source vocab size: {len(src_vocab)}')\n",
    "print(f'Target vocab size: {len(tgt_vocab)}')\n",
    "print('First few source vocab chars:', src_vocab[:10])\n",
    "print('First few target vocab chars:', tgt_vocab[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T21:18:38.516793Z",
     "iopub.status.busy": "2025-10-30T21:18:38.516498Z",
     "iopub.status.idle": "2025-10-30T21:18:38.679835Z",
     "shell.execute_reply": "2025-10-30T21:18:38.679012Z",
     "shell.execute_reply.started": "2025-10-30T21:18:38.516761Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source batch shape: torch.Size([128, 48])\n",
      "Target batch shape: torch.Size([128, 48])\n"
     ]
    }
   ],
   "source": [
    "# Let's wrap our data in a PyTorch Dataset for easy batching and training\n",
    "class TransliterationDataset(Dataset):\n",
    "    def __init__(self, pairs, src_char2idx, tgt_char2idx, max_len=32):\n",
    "        self.pairs = pairs\n",
    "        self.src_char2idx = src_char2idx\n",
    "        self.tgt_char2idx = tgt_char2idx\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    def encode_seq(self, seq, char2idx):\n",
    "        # Add SOS and EOS tokens, then pad to max_len\n",
    "        idxs = [char2idx[SOS_token]] + [char2idx[c] for c in seq] + [char2idx[EOS_token]]\n",
    "        if len(idxs) < self.max_len:\n",
    "            idxs += [char2idx[PAD_token]] * (self.max_len - len(idxs))\n",
    "        else:\n",
    "            idxs = idxs[:self.max_len]\n",
    "        return torch.tensor(idxs, dtype=torch.long)\n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.pairs[idx]\n",
    "        src_encoded = self.encode_seq(src, self.src_char2idx)\n",
    "        tgt_encoded = self.encode_seq(tgt, self.tgt_char2idx)\n",
    "        return src_encoded, tgt_encoded\n",
    "\n",
    "# Example usage: create a dataset and a dataloader\n",
    "train_dataset = TransliterationDataset(pairs, src_char2idx, tgt_char2idx, max_len=MAX_LEN)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "src_batch, tgt_batch = next(iter(train_loader))\n",
    "print('Source batch shape:', src_batch.shape)\n",
    "print('Target batch shape:', tgt_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T21:18:38.681014Z",
     "iopub.status.busy": "2025-10-30T21:18:38.680766Z",
     "iopub.status.idle": "2025-10-30T21:18:38.987423Z",
     "shell.execute_reply": "2025-10-30T21:18:38.986654Z",
     "shell.execute_reply.started": "2025-10-30T21:18:38.680994Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (src_embedding): Embedding(29, 128, padding_idx=0)\n",
      "  (tgt_embedding): Embedding(681, 128, padding_idx=0)\n",
      "  (encoder): GRU(128, 256, num_layers=2, batch_first=True)\n",
      "  (decoder): GRU(128, 256, num_layers=2, batch_first=True)\n",
      "  (fc_out): Linear(in_features=256, out_features=681, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Defining our flexible Seq2Seq model: you can change embedding size, hidden size, RNN type, and layers\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, emb_dim=64, hidden_dim=128, rnn_type='LSTM', num_layers=1):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.rnn_type = rnn_type\n",
    "        self.num_layers = num_layers\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, emb_dim, padding_idx=src_char2idx[PAD_token])\n",
    "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, emb_dim, padding_idx=tgt_char2idx[PAD_token])\n",
    "        rnn_cls = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[rnn_type]\n",
    "        self.encoder = rnn_cls(emb_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.decoder = rnn_cls(emb_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, tgt_vocab_size)\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        src_emb = self.src_embedding(src)\n",
    "        # Encoder: process the input sequence\n",
    "        enc_outputs, enc_hidden = self.encoder(src_emb)\n",
    "        # Decoder: generate output sequence one token at a time\n",
    "        tgt_len = tgt.size(1)\n",
    "        outputs = torch.zeros(batch_size, tgt_len, len(tgt_vocab)).to(src.device)\n",
    "        input = tgt[:,0]  # Start with SOS token\n",
    "        hidden = enc_hidden\n",
    "        for t in range(1, tgt_len):\n",
    "            input_emb = self.tgt_embedding(input).unsqueeze(1)  # (batch, 1, emb_dim)\n",
    "            dec_output, hidden = self.decoder(input_emb, hidden)\n",
    "            out_token = self.fc_out(dec_output.squeeze(1))\n",
    "            outputs[:,t,:] = out_token\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = out_token.argmax(1)\n",
    "            input = tgt[:,t] if teacher_force else top1\n",
    "        return outputs\n",
    "\n",
    "# Create the model (feel free to change hyperparameters)\n",
    "model = Seq2Seq(\n",
    "    src_vocab_size=len(src_vocab),\n",
    "    tgt_vocab_size=len(tgt_vocab),\n",
    "    emb_dim=EMB_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    rnn_type=RNN_TYPE,\n",
    "    num_layers=NUM_LAYERS\n",
    ").to(DEVICE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T21:18:38.988458Z",
     "iopub.status.busy": "2025-10-30T21:18:38.988236Z",
     "iopub.status.idle": "2025-10-30T21:20:14.506023Z",
     "shell.execute_reply": "2025-10-30T21:20:14.505160Z",
     "shell.execute_reply.started": "2025-10-30T21:18:38.988439Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Loss: 6.5271\n",
      "Batch 10, Loss: 5.4728\n",
      "Batch 20, Loss: 5.3213\n",
      "Batch 30, Loss: 5.2940\n",
      "Batch 40, Loss: 5.2104\n",
      "Batch 50, Loss: 5.1846\n",
      "Batch 60, Loss: 5.0204\n",
      "Batch 70, Loss: 5.1410\n",
      "Batch 80, Loss: 4.7941\n",
      "Batch 90, Loss: 4.7589\n",
      "Batch 100, Loss: 4.4605\n",
      "Batch 110, Loss: 4.5106\n",
      "Batch 120, Loss: 4.2648\n",
      "Batch 130, Loss: 3.9539\n",
      "Batch 140, Loss: 4.2836\n",
      "Batch 150, Loss: 4.2027\n",
      "Batch 160, Loss: 3.8458\n",
      "Batch 170, Loss: 3.8116\n",
      "Batch 180, Loss: 4.0138\n",
      "Batch 190, Loss: 3.9015\n",
      "Batch 200, Loss: 3.5650\n",
      "Batch 210, Loss: 3.6606\n",
      "Batch 220, Loss: 3.7727\n",
      "Batch 230, Loss: 3.6330\n",
      "Batch 240, Loss: 4.4366\n",
      "Batch 250, Loss: 3.4200\n",
      "Batch 260, Loss: 3.5014\n",
      "Batch 270, Loss: 3.9123\n",
      "Batch 280, Loss: 3.3753\n",
      "Batch 290, Loss: 3.3913\n",
      "Batch 300, Loss: 3.3184\n",
      "Batch 310, Loss: 3.1361\n",
      "Batch 320, Loss: 3.3656\n",
      "Batch 330, Loss: 3.5291\n",
      "Batch 340, Loss: 3.4821\n",
      "Batch 350, Loss: 3.3127\n",
      "Batch 360, Loss: 3.1455\n",
      "Batch 370, Loss: 3.0067\n",
      "Batch 380, Loss: 3.7520\n",
      "Batch 390, Loss: 2.9772\n",
      "Batch 400, Loss: 2.9951\n",
      "Batch 410, Loss: 2.9202\n",
      "Batch 420, Loss: 2.9330\n",
      "Batch 430, Loss: 3.1761\n",
      "Batch 440, Loss: 2.8924\n",
      "Batch 450, Loss: 2.9323\n",
      "Batch 460, Loss: 2.9423\n",
      "Batch 470, Loss: 3.7887\n",
      "Batch 480, Loss: 3.5775\n",
      "Batch 490, Loss: 2.7783\n",
      "Batch 500, Loss: 2.7677\n",
      "Batch 510, Loss: 2.6772\n",
      "Batch 520, Loss: 3.3423\n",
      "Batch 530, Loss: 2.9274\n",
      "Batch 540, Loss: 2.8368\n",
      "Batch 550, Loss: 3.7773\n",
      "Batch 560, Loss: 4.2230\n",
      "Batch 570, Loss: 2.7654\n",
      "Batch 580, Loss: 2.6256\n",
      "Batch 590, Loss: 3.5828\n",
      "Batch 600, Loss: 2.5705\n",
      "Batch 610, Loss: 2.5498\n",
      "Batch 620, Loss: 2.7489\n",
      "Batch 630, Loss: 2.7835\n",
      "Batch 640, Loss: 2.6452\n",
      "Batch 650, Loss: 2.5871\n",
      "Batch 660, Loss: 3.5762\n",
      "Batch 670, Loss: 2.7155\n",
      "Batch 680, Loss: 2.4062\n",
      "Batch 690, Loss: 2.4978\n",
      "Batch 700, Loss: 2.4581\n",
      "Batch 710, Loss: 2.6580\n",
      "Batch 720, Loss: 2.3758\n",
      "Batch 730, Loss: 2.4763\n",
      "Batch 740, Loss: 2.2875\n",
      "Batch 750, Loss: 2.4179\n",
      "Batch 760, Loss: 2.3146\n",
      "Batch 770, Loss: 2.4043\n",
      "Batch 780, Loss: 2.2185\n",
      "Batch 790, Loss: 2.2171\n",
      "Batch 800, Loss: 2.4031\n",
      "Batch 810, Loss: 2.6540\n",
      "Batch 820, Loss: 2.2795\n",
      "Batch 830, Loss: 2.3530\n",
      "Batch 840, Loss: 2.3634\n",
      "Batch 850, Loss: 2.2223\n",
      "Batch 860, Loss: 2.3105\n",
      "Batch 870, Loss: 2.4673\n",
      "Batch 880, Loss: 2.3388\n",
      "Batch 890, Loss: 2.4332\n",
      "Batch 900, Loss: 2.1846\n",
      "Batch 910, Loss: 2.5257\n",
      "Batch 920, Loss: 2.0723\n",
      "Batch 930, Loss: 2.3556\n",
      "Batch 940, Loss: 2.5018\n",
      "Batch 950, Loss: 2.1815\n",
      "Batch 960, Loss: 2.0818\n",
      "Batch 970, Loss: 2.4263\n",
      "Batch 980, Loss: 2.1897\n",
      "Batch 990, Loss: 2.1560\n",
      "Batch 1000, Loss: 2.1975\n",
      "Concluding Training Process\n"
     ]
    }
   ],
   "source": [
    "# Let's train for a few batches to check everything works\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tgt_char2idx[PAD_token])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()\n",
    "for batch_idx, (src_batch, tgt_batch) in enumerate(train_loader):\n",
    "    src_batch = src_batch.to(DEVICE)\n",
    "    tgt_batch = tgt_batch.to(DEVICE)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(src_batch, tgt_batch, teacher_forcing_ratio=0.5)\n",
    "    # outputs: (batch, tgt_len, tgt_vocab_size)\n",
    "    # tgt_batch: (batch, tgt_len)\n",
    "    loss = loss_fn(outputs.view(-1, len(tgt_vocab)), tgt_batch.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if batch_idx % 10 == 0:\n",
    "        print(f'Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "    if batch_idx == 1000:\n",
    "        print('Concluding Training Process')\n",
    "        break  # Tested with 20 batches on cpu locally, 1000 here on Kaggle as it has the P100 GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "\n",
    "We can gradually notice a decrease in the loss function value, which indicates progress in the model training. Thus, this particular setup is quite effective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytic Formulas: Computations and Parameters\n",
    "\n",
    "Let:\n",
    "- $E$ = embedding size\n",
    "- $H$ = hidden size\n",
    "- $L$ = sequence length\n",
    "- $V$ = vocabulary size (source/target)\n",
    "\n",
    "**Total Parameters:**\n",
    "- Embedding layers: $2 \\times (V \\times E)$\n",
    "- Encoder RNN (LSTM): $4 \\times [(E + H) \\times H + H]$ (weights and biases)\n",
    "- Decoder RNN (LSTM): $4 \\times [(E + H) \\times H + H]$\n",
    "- Output FC: $H \\times V + V$\n",
    "- **Total:**\n",
    "  $$P = 2VE + 8(E+H)H + 8H + HV + V$$\n",
    "\n",
    "**Total Computations per Forward Pass:**\n",
    "- For each time step in encoder and decoder (L steps each):\n",
    "  - LSTM cell: $4 \\times [(E + H) \\times H]$ multiplies per step\n",
    "- Output FC: $H \\times V$ per output step\n",
    "- **Total:**\n",
    "  $$C = L \\times [4(E+H)H]_{enc} + L \\times [4(E+H)H + HV]_{dec}$$\n",
    "\n",
    "Assume 1 layer each, $E=64$, $H=128$, $L=16$, $V=50$ for example.\n",
    "\n",
    "We can change $E$, $H$, $L$, $V$ in the model and recompute these values.\n",
    "\n",
    "The below python code answers the questions based on the usage of cpu or gpu(cuda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T21:20:14.507615Z",
     "iopub.status.busy": "2025-10-30T21:20:14.507129Z",
     "iopub.status.idle": "2025-10-30T21:20:14.516013Z",
     "shell.execute_reply": "2025-10-30T21:20:14.515157Z",
     "shell.execute_reply.started": "2025-10-30T21:20:14.507588Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model analytic summary for current settings:\n",
      "Embedding size (E): 128\n",
      "Hidden size (H): 256\n",
      "Sequence length (L): 48\n",
      "Vocabulary size (V): 29\n",
      "Total number of parameters: 803,357\n",
      "Total number of computations per forward pass: 38,105,088\n",
      "\n",
      "Formulas used:\n",
      "Parameters: 2VE + 8(E+H)H + 8H + HV + V\n",
      "Computations: 8L(E+H)H + LHV\n"
     ]
    }
   ],
   "source": [
    "# --- Analytic calculation of parameters and computations for current model settings ---\n",
    "\n",
    "def calc_params_and_computations(E, H, L, V):\n",
    "    # Total parameters\n",
    "    embedding_params = 2 * V * E\n",
    "    encoder_params = 4 * ((E + H) * H + H)\n",
    "    decoder_params = 4 * ((E + H) * H + H)\n",
    "    output_params = H * V + V\n",
    "    total_params = embedding_params + encoder_params + decoder_params + output_params\n",
    "\n",
    "    # Total computations per forward pass (batch size = 1)\n",
    "    encoder_computations = L * 4 * (E + H) * H\n",
    "    decoder_computations = L * 4 * (E + H) * H\n",
    "    output_computations = L * H * V\n",
    "    total_computations = encoder_computations + decoder_computations + output_computations\n",
    "\n",
    "    return total_params, total_computations\n",
    "\n",
    "E = EMB_DIM\n",
    "H = HIDDEN_DIM\n",
    "L = MAX_LEN\n",
    "V = len(src_vocab)  # or len(tgt_vocab), they are the same\n",
    "\n",
    "params, computations = calc_params_and_computations(E, H, L, V)\n",
    "\n",
    "print(\"Model analytic summary for current settings:\")\n",
    "print(f\"Embedding size (E): {E}\")\n",
    "print(f\"Hidden size (H): {H}\")\n",
    "print(f\"Sequence length (L): {L}\")\n",
    "print(f\"Vocabulary size (V): {V}\")\n",
    "print(f\"Total number of parameters: {params:,}\")\n",
    "print(f\"Total number of computations per forward pass: {computations:,}\")\n",
    "\n",
    "# For reference, formulas used:\n",
    "print(\"\\nFormulas used:\")\n",
    "print(\"Parameters: 2VE + 8(E+H)H + 8H + HV + V\")\n",
    "print(\"Computations: 8L(E+H)H + LHV\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8613774,
     "sourceId": 13560840,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
